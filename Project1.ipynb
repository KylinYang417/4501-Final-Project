{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Project Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all import statements needed for the project, for example:\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import requests\n",
    "import re \n",
    "import time \n",
    "import urllib.request\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def url_date_ge(a: str, b: str) -> bool:\n",
    "  \"\"\"Return a boolean that tells whether the input date a is greater than or equal to date b\"\"\"\n",
    "  return (int(a[:4]), int(a[-2:])) >= (int(b[:4]), int(b[-2:]))\n",
    "\n",
    "def url_date_le(a: str, b: str) -> bool:\n",
    "  \"\"\"Return a boolean that tells whether the input date a is less than or equal to date b\"\"\"\n",
    "  return (int(a[:4]), int(a[-2:])) <= (int(b[:4]), int(b[-2:]))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List out url that will be using\n",
    "uber_url = 'https://drive.google.com/file/d/1F7D82w1D5151GXCR6BTEk7mNQ_YnPNDk/view?usp=sharing'\n",
    "weather_url = 'https://drive.google.com/drive/folders/1I_Cj3RFHRGcQjb5Gas06buqRbKodIwKC?usp=sharing'\n",
    "taxi_url = 'https://www.nyc.gov/site/tlc/about/tlc-trip-record-data.page'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_yellow_taxi_parquet(taxi_url: str, \n",
    "                                 from_date: str = '2009-01', to_date: str = '2015-06',\n",
    "                                 taxi_par_save_dir: str = 'yellow_taxi',\n",
    "                                 max_waiting_time: float = 10 * 60)  -> None:\n",
    "  \"\"\"Download the yellow taxi parquet files from taxi_url.\"\"\"\n",
    "  # Make a request to the webpage\n",
    "  response = requests.get(taxi_url)\n",
    "  # Use BeautifulSoup to parse the HTML content of the webpage\n",
    "  soup = BeautifulSoup(response.content, 'html.parser')\n",
    "  data_urls = [tag['href'] for tag in soup.find_all(attrs={'title': 'Yellow Taxi Trip Records'})]\n",
    "  inrange_urls = [_url for _url in data_urls \n",
    "                if url_date_ge(re.search(r'(\\d+-\\d+)', _url).group(0), from_date) \n",
    "                and url_date_le(re.search(r'(\\d+-\\d+)', _url).group(0), to_date)\n",
    "                ]\n",
    "  \n",
    "  # Create folder\n",
    "  if not os.path.exists(taxi_par_save_dir):\n",
    "    os.mkdir(taxi_par_save_dir)\n",
    "\n",
    "  done = False\n",
    "  st = time.time()\n",
    "  while not done and time.time() - st < max_waiting_time:\n",
    "    try:\n",
    "      for _url in inrange_urls:\n",
    "        save_file = os.path.join(taxi_par_save_dir, _url[_url.index('yellow'):])\n",
    "        if not os.path.exists(save_file):\n",
    "          print(f'downloading to {save_file}...')\n",
    "          urllib.request.urlretrieve(_url, save_file)\n",
    "          print('finished.')\n",
    "        else:\n",
    "          print(f'{save_file} already downloaded.')\n",
    "      \n",
    "      done = True\n",
    "      print('All downloads finished.')\n",
    "    except:\n",
    "      continue \n",
    "  \n",
    "  if not done:\n",
    "    print('Not all files downloaded. Might be insufficient max waiting time. You may re-run this function.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute function to download yellow taxi parquet files\n",
    "download_yellow_taxi_parquet(taxi_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_unzip_shapefile(taxi_url: str, shapefile_save_dir: str = 'assets') -> None:\n",
    "  \"\"\"Download and unzip the Taxi Zone Shapefile from taxi_url.\"\"\"\n",
    "  # Make a request to the webpage\n",
    "  response = requests.get(taxi_url)\n",
    "  # Use BeautifulSoup to parse the HTML content of the webpage\n",
    "  soup = BeautifulSoup(response.content, 'html.parser')\n",
    "  shapefile_url = soup.find(string='Taxi Zone Shapefile').parent['href']\n",
    "\n",
    "  if not os.path.exists(shapefile_save_dir):\n",
    "    os.mkdir(shapefile_save_dir)\n",
    "  save_file = os.path.join(shapefile_save_dir, 'taxi_zones.zip')\n",
    "  if not os.path.exists(save_file):\n",
    "    urllib.request.urlretrieve(shapefile_url, save_file)\n",
    "  if not os.path.exists(os.path.join('.', 'taxi_zones.shp')):\n",
    "    !unzip {save_file} \n",
    "  print('Downloaded shapefile.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute the function to download and unzip the Taxi Zone Shapefile from taxi_url\n",
    "download_unzip_shapefile(taxi_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
