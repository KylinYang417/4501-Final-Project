{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Project Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all import statements needed for the project\n",
    "import os \n",
    "import requests\n",
    "import re \n",
    "import time \n",
    "import urllib.request\n",
    "from bs4 import BeautifulSoup\n",
    "from joblib import Parallel, delayed\n",
    "import math \n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "import geopandas as gpd\n",
    "import pyarrow.parquet as pq"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import requests\n",
    "import re \n",
    "import time \n",
    "import urllib.request\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def url_date_ge(a: str, b: str) -> bool:\n",
    "  \"\"\"Return a boolean that tells whether the input date a is greater than or equal to date b\"\"\"\n",
    "  return (int(a[:4]), int(a[-2:])) >= (int(b[:4]), int(b[-2:]))\n",
    "\n",
    "def url_date_le(a: str, b: str) -> bool:\n",
    "  \"\"\"Return a boolean that tells whether the input date a is less than or equal to date b\"\"\"\n",
    "  return (int(a[:4]), int(a[-2:])) <= (int(b[:4]), int(b[-2:]))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List out url that will be using\n",
    "uber_url = 'https://drive.google.com/file/d/1F7D82w1D5151GXCR6BTEk7mNQ_YnPNDk/view?usp=sharing'\n",
    "weather_url = 'https://drive.google.com/drive/folders/1I_Cj3RFHRGcQjb5Gas06buqRbKodIwKC?usp=sharing'\n",
    "taxi_url = 'https://www.nyc.gov/site/tlc/about/tlc-trip-record-data.page'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_yellow_taxi_parquet(taxi_url: str, \n",
    "                                 from_date: str = '2009-01', to_date: str = '2015-06',\n",
    "                                 taxi_par_save_dir: str = 'yellow_taxi',\n",
    "                                 max_waiting_time: float = 10 * 60)  -> None:\n",
    "  \"\"\"Download the yellow taxi parquet files from taxi_url.\"\"\"\n",
    "  # Make a request to the webpage\n",
    "  response = requests.get(taxi_url)\n",
    "  # Use BeautifulSoup to parse the HTML content of the webpage\n",
    "  soup = BeautifulSoup(response.content, 'html.parser')\n",
    "  data_urls = [tag['href'] for tag in soup.find_all(attrs={'title': 'Yellow Taxi Trip Records'})]\n",
    "  inrange_urls = [_url for _url in data_urls \n",
    "                if url_date_ge(re.search(r'(\\d+-\\d+)', _url).group(0), from_date) \n",
    "                and url_date_le(re.search(r'(\\d+-\\d+)', _url).group(0), to_date)\n",
    "                ]\n",
    "  \n",
    "  # Create folder\n",
    "  if not os.path.exists(taxi_par_save_dir):\n",
    "    os.mkdir(taxi_par_save_dir)\n",
    "\n",
    "  done = False\n",
    "  st = time.time()\n",
    "  while not done and time.time() - st < max_waiting_time:\n",
    "    try:\n",
    "      for _url in inrange_urls:\n",
    "        save_file = os.path.join(taxi_par_save_dir, _url[_url.index('yellow'):])\n",
    "        if not os.path.exists(save_file):\n",
    "          print(f'downloading to {save_file}...')\n",
    "          urllib.request.urlretrieve(_url, save_file)\n",
    "          print('finished.')\n",
    "        else:\n",
    "          print(f'{save_file} already downloaded.')\n",
    "      \n",
    "      done = True\n",
    "      print('All downloads finished.')\n",
    "    except:\n",
    "      continue \n",
    "  \n",
    "  if not done:\n",
    "    print('Not all files downloaded. Might be insufficient max waiting time. You may re-run this function.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute function to download yellow taxi parquet files\n",
    "download_yellow_taxi_parquet(taxi_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_unzip_shapefile(taxi_url: str, shapefile_save_dir: str = 'assets') -> None:\n",
    "  \"\"\"Download and unzip the Taxi Zone Shapefile from taxi_url.\"\"\"\n",
    "  # Make a request to the webpage\n",
    "  response = requests.get(taxi_url)\n",
    "  # Use BeautifulSoup to parse the HTML content of the webpage\n",
    "  soup = BeautifulSoup(response.content, 'html.parser')\n",
    "  shapefile_url = soup.find(string='Taxi Zone Shapefile').parent['href']\n",
    "\n",
    "  if not os.path.exists(shapefile_save_dir):\n",
    "    os.mkdir(shapefile_save_dir)\n",
    "  save_file = os.path.join(shapefile_save_dir, 'taxi_zones.zip')\n",
    "  if not os.path.exists(save_file):\n",
    "    urllib.request.urlretrieve(shapefile_url, save_file)\n",
    "  if not os.path.exists(os.path.join('.', 'taxi_zones.shp')):\n",
    "    !unzip {save_file} \n",
    "  print('Downloaded shapefile.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute the function to download and unzip the Taxi Zone Shapefile from taxi_url\n",
    "download_unzip_shapefile(taxi_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "from joblib import Parallel, delayed\n",
    "import math \n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "import geopandas as gpd\n",
    "import pyarrow.parquet as pq\n",
    "\n",
    "class Part1:\n",
    "  def __init__(self, \n",
    "               taxi_par_save_dir='yellow_taxi',\n",
    "               uber_path='uber_rides_sample.csv',\n",
    "               shapefile_save_dir='.',\n",
    "               processed_path='processed'):\n",
    "    self.taxi_par_save_dir = taxi_par_save_dir\n",
    "    self.shapefile_save_dir = shapefile_save_dir\n",
    "    self.shp_path = os.path.join(shapefile_save_dir, 'taxi_zones.shp')\n",
    "    self.uber_path = uber_path\n",
    "    self.processed_path = processed_path\n",
    "    gdf = gpd.read_file(self.shp_path)\n",
    "    gdf['centroid'] = gdf.geometry.centroid.to_crs(4326)\n",
    "    gdf = gdf.set_index('OBJECTID')\n",
    "    self.gdf = gdf\n",
    "    self.pars = os.listdir(taxi_par_save_dir)\n",
    "    self.pars = sorted(self.pars, key=lambda x: (int(x[x.index('.')-7: x.index('.')-3]), \n",
    "                                            int(x[x.index('.')-2: x.index('.')])))\n",
    "    self.box = (min_longitude, max_longitude, min_latitude, max_latitude) = (-74.242330, -73.717047, \n",
    "                                                                             40.560445, 40.908524)\n",
    "    \n",
    "    # cols\n",
    "    self.cols_mapping = {'vendor_name': 'vendor_id',\n",
    "                        'VendorID': 'vendor_id',\n",
    "                        'tpep_pickup_datetime': 'pickup_datetime',\n",
    "                        'Trip_Pickup_DateTime': 'pickup_datetime',\n",
    "                        'tpep_dropoff_datetime': 'dropoff_datetime',\n",
    "                        'Trip_Dropoff_DateTime': 'dropoff_datetime',\n",
    "                        'Passenger_Count': 'passenger_count',\n",
    "                        'Trip_Distance': 'trip_distance',\n",
    "                        'Start_Lon': 'pickup_longitude',\n",
    "                        'Start_Lat': 'pickup_latitude',\n",
    "                        'End_Lon': 'dropoff_longitude', \n",
    "                        'End_Lat': 'dropoff_latitude',\n",
    "                        'Payment_Type': 'payment_type',\n",
    "                        'Tip_Amt': 'tip_amount',\n",
    "                        'Tolls_Amt': 'tolls_amount',\n",
    "                        'Total_Amt': 'total_amount',\n",
    "                        'Fare_Amt': 'fare_amount'\n",
    "                        }\n",
    "    self.cols = ['vendor_id', 'pickup_datetime', 'dropoff_datetime', 'passenger_count',\n",
    "            'trip_distance', 'pickup_longitude', 'pickup_latitude', 'dropoff_longitude',\n",
    "            'dropoff_latitude', 'payment_type', 'tip_amount', 'tolls_amount', \n",
    "            'total_amount', 'fare_amount']\n",
    "\n",
    "    self.types = {'vendor_id': str, 'pickup_datetime': \"datetime64[ns]\", 'dropoff_datetime': \"datetime64[ns]\", \n",
    "            'passenger_count': np.int8, 'trip_distance': np.float32, 'pickup_longitude': np.float32,\n",
    "            'pickup_latitude': np.float32, 'dropoff_longitude': np.float32, 'dropoff_latitude': np.float32,\n",
    "            'payment_type': str, 'tip_amount': np.float32, 'tolls_amount': np.float32, \n",
    "            'total_amount': np.float32, 'fare_amount': np.float32}        \n",
    "  \n",
    "  def process_yellow_taxi_parquets(self)-> pd.DataFrame:\n",
    "    \"\"\"Reture a dataframe of all processed yellow taxi parquet files\"\"\"\n",
    "    if not os.path.exists(self.processed_path):\n",
    "      os.mkdir(self.processed_path)\n",
    "    clean_sampled_df_list = [0] * len(self.pars)\n",
    "    sample_ratio = self._determine_sample_ratio()\n",
    "    for i, par in enumerate(self.pars):\n",
    "      clean_sampled_df = self._process_one_yellow_taxi_parquet(par)\n",
    "      clean_sampled_df_list[i] = clean_sampled_df\n",
    "    \n",
    "    processed_all_yellow_taxi_df = pd.concat(clean_sampled_df_list)\n",
    "    return processed_all_yellow_taxi_df\n",
    "  \n",
    "  def _determine_sample_ratio(self) -> int:\n",
    "    \"\"\"Return a ratio, used for sampling yellow taxi data\"\"\"\n",
    "    uber_df = pd.read_csv(self.uber_path)\n",
    "    uber_rows = len(uber_df)\n",
    "    num_rows = 0\n",
    "    for i in range(len(self.pars)):\n",
    "      data_path = os.path.join(self.taxi_par_save_dir, self.pars[i])\n",
    "      pf = pq.ParquetFile(data_path)\n",
    "      # count the number of rows in the file\n",
    "      num_rows += pf.metadata.num_rows\n",
    "    self.sample_ratio = uber_rows / num_rows\n",
    "    return self.sample_ratio\n",
    "  \n",
    "  def _process_one_yellow_taxi_parquet(self, parquet_file: str) -> pd.DataFrame:\n",
    "    \"\"\"Return a processed dataset of the input parquet file\n",
    "    \n",
    "    Parameter:\n",
    "    parquet_file: one single yellow taxi parquet file\n",
    "    \"\"\"\n",
    "    data_path = os.path.join(self.taxi_par_save_dir, parquet_file)\n",
    "    large_parquet = pq.ParquetFile(data_path)\n",
    "    \n",
    "    prod_list = []\n",
    "    for trips in large_parquet.iter_batches(batch_size=100000):\n",
    "      trips = trips.to_pandas()\n",
    "      # Rename the columns by predefined column name in cols_mapping\n",
    "      prod_trips = trips.rename(columns={k: v for k, v in self.cols_mapping.items() if k in trips.columns})\n",
    "      # Lookup for latitude and longitude and add to dataframe\n",
    "      if 'DOLocationID' in trips.columns:\n",
    "        prod_trips = self.lookup_longlat('D', self.gdf, prod_trips)\n",
    "      if 'PULocationID' in trips.columns:\n",
    "        prod_trips = self.lookup_longlat('P', self.gdf, prod_trips)\n",
    "      \n",
    "      prod_trips = prod_trips[self.cols].astype(self.types).copy() # only a subset of columns are needed\n",
    "      prod_trips = prod_trips.dropna(axis=0, how='any', subset=['dropoff_longitude', 'dropoff_latitude', 'pickup_longitude', 'pickup_latitude'])\n",
    "      to_drop1 = prod_trips.apply(lambda row: not self._if_inside_box(row['dropoff_longitude'], row['dropoff_latitude']), axis=1)\n",
    "      to_drop2 = prod_trips.apply(lambda row: not self._if_inside_box(row['pickup_longitude'], row['pickup_latitude']), axis=1)\n",
    "      prod_trips = prod_trips.drop(prod_trips[to_drop1 + to_drop2].index)\n",
    "      prod_trips['trip_distance'] = prod_trips.apply(lambda row: \n",
    "                                                    self.calc_distance(row['pickup_longitude'], row['pickup_latitude'], row['dropoff_longitude'], row['dropoff_latitude']) \n",
    "                                                    if np.isnan(row['trip_distance']) \n",
    "                                                    else row['trip_distance'],\n",
    "                                                    axis=1)\n",
    "      prod_trips = prod_trips.sample(frac=self.sample_ratio)\n",
    "      prod_list.append(prod_trips)\n",
    "    prod_trips = pd.concat(prod_list)\n",
    "\n",
    "    par_savepath = os.path.join(self.processed_path, parquet_file)\n",
    "    if not os.path.exists(par_savepath):\n",
    "      prod_trips.to_parquet(par_savepath)\n",
    "    return prod_trips\n",
    "\n",
    "  def _if_inside_box(self, long: float, lat: float) -> bool:\n",
    "    \"\"\"Return a boolean that determines whether the trips start and/or end inside of the\n",
    "    latitude/longitude coordinate box: (40.560445, -74.242330) and (40.908524, -73.717047).\n",
    "\n",
    "    Parameter:\n",
    "    long: float, the longtitude of the trip\n",
    "    lat: float, the latitude of the trip\n",
    "    \"\"\"\n",
    "\n",
    "    min_longitude, max_longitude, min_latitude, max_latitude = self.box \n",
    "    return min_longitude <= long <= max_longitude and min_latitude <= lat <= max_latitude\n",
    "  \n",
    "  def lookup_longlat(self, loc_type: str, gdf: pd.DataFrame, prod_trips: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Return a processed dataframe for yellow taxi, \n",
    "    Add addtional column to record corresponding latitude and longitude, for location ID \n",
    "\n",
    "    Parameter:\n",
    "    loc_type: a string value, to indicate whether the input dataframe is prickup or dropoff, can be 'D' or 'P'\n",
    "    gdf: a dataframe of Taxi Zone Shapefile, one location ID corresponds to one set of latitude and longitude\n",
    "    prod_trips: a dataframe of yellow taxi location ID\n",
    "    \"\"\"\n",
    "\n",
    "    # Create column names according to input as pickup or dropoff\n",
    "    col_name = 'DOLocationID' if loc_type == 'D' else 'PULocationID'\n",
    "    add_name = 'dropoff' if loc_type == 'D' else 'pickup'\n",
    "    # Lookup for latitude and longitude for the location ID\n",
    "    found, lookup = self.lookup_with_id(prod_trips[col_name], gdf)\n",
    "    long, lat = found['centroid'].x, found['centroid'].y\n",
    "    # Create new empty columns\n",
    "    prod_trips[f'{add_name}_longitude'] = np.nan\n",
    "    prod_trips[f'{add_name}_latitude'] = np.nan\n",
    "    # Fill in the latitude and longitude value\n",
    "    prod_trips.loc[lookup.index, f'{add_name}_longitude'] = long.values\n",
    "    prod_trips.loc[lookup.index, f'{add_name}_latitude'] = lat.values\n",
    "    return prod_trips \n",
    "  \n",
    "  def lookup_with_id(self, sr: list, df: pd.DataFrame) -> tuple:\n",
    "    \"\"\"Look up for latitude and longitude of a given pickup and dropoff location ID\n",
    "    Return a tuple containing the set of latitude and longitude, \n",
    "    and the valid location ID from yellow taxi data that corresponds to it \n",
    "\n",
    "    Parameter:\n",
    "    sr: a series of yellow taxi pickup and dropoff location ID\n",
    "    df: a dataframe of Taxi Zone Shapefile, one location ID corresponds to one set of latitude and longitude   \n",
    "    \"\"\"\n",
    "    # filter for valid location ID in sr\n",
    "    lookup = sr[sr.isin(df.index)]\n",
    "    return df.loc[lookup], lookup\n",
    "  \n",
    "  def deg2rad(self, deg: float) -> float:\n",
    "    \"\"\"Convert a degree to radius\n",
    "    Parameter:\n",
    "    deg: float\n",
    "    \"\"\"\n",
    "    return deg * (math.pi/180)\n",
    "\n",
    "  def calc_distance(self, plong: float, plat: float, dlong: float, dlat: float) -> float:\n",
    "    \"\"\"Return a calculated distance in km between two latitude-longitude points\n",
    "    Parameter:\n",
    "    plong: float, pickup longitude\n",
    "    plat: float, pickup latitude\n",
    "    dlong: float, dropoff longitude\n",
    "    dlat: float, dropoff latitude\n",
    "    \"\"\"\n",
    "    # The following code was adapted from this StackOverflow \n",
    "    # answer from user1921: https://stackoverflow.com/questions/27928/calculate-distance-between-two-latitude-longitude-points-haversine-formula\n",
    "    R = 6371 # Radius of the earth in km\n",
    "    long = plong - dlong\n",
    "    lat = plat - dlat\n",
    "    lat_deg = self.deg2rad(lat)\n",
    "    long_deg = self.deg2rad(long)\n",
    "    a = (math.sin(lat_deg/2) * math.sin(lat_deg/2) \n",
    "    + math.cos(self.deg2rad(plat)) * math.cos(self.deg2rad(dlat)) \n",
    "    * math.sin(long_deg/2) * math.sin(long_deg/2)\n",
    "    )\n",
    "      \n",
    "    c = 2 * math.atan2(math.sqrt(a), math.sqrt(1-a)); \n",
    "    d = R * c # Distance in km\n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an instance of the defined class to process yellow taxi data\n",
    "p1 = Part1()\n",
    "processed_all_yellow_taxi_df = p1.process_yellow_taxi_parquets()\n",
    "# Save data to csv file\n",
    "processed_all_yellow_taxi_df.to_csv('processed_yellow_taxi.csv')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Storing Cleaned Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "def create_yellow_taxi() -> None:\n",
    "    \"\"\"Create a SQLite database, create and populate a table for sampled dataset of yellow taxi trips\"\"\"\n",
    "    yellow_taxi_col_type = {'vendor_id': str, \n",
    "                'passenger_count': np.int8, 'trip_distance': np.float32, 'pickup_longitude': np.float32,\n",
    "                'pickup_latitude': np.float32, 'dropoff_longitude': np.float32, 'dropoff_latitude': np.float32,\n",
    "                'payment_type': str, 'tip_amount': np.float32, 'tolls_amount': np.float32, \n",
    "                'total_amount': np.float32, 'fare_amount': np.float32} \n",
    "\n",
    "    # Read the CSV file into a pandas DataFrame\n",
    "    csv_file = 'processed_yellow_taxi.csv'\n",
    "    ydf = pd.read_csv(csv_file, dtype=yellow_taxi_col_type, parse_dates=['pickup_datetime', 'dropoff_datetime'])\n",
    "\n",
    "    # Connect to the SQLite database\n",
    "    engine = create_engine('sqlite:///part2.sqlite3')\n",
    "\n",
    "    # Write the DataFrame to an SQLite table\n",
    "    table_name = 'yellow_taxi'\n",
    "    ydf.to_sql(table_name, engine, if_exists='replace', index=False)\n",
    "\n",
    "    engine.dispose()\n",
    "\n",
    "def create_uber() -> None:\n",
    "    \"\"\"Create a SQLite database, create and populate a table for sampled dataset of uber trips\"\"\"\n",
    "    f32 = np.float32\n",
    "    uber_col_type = {\n",
    "        'fare_amount': f32, 'pickup_longitude': f32, 'pickup_latitude': f32, 'dropoff_longitude': f32, 'dropoff_latitude': f32,\n",
    "        'passenger_count': int, 'distance': f32\n",
    "    }            \n",
    "\n",
    "    # Read the CSV file into a pandas DataFrame\n",
    "    csv_file = 'processed_uber.csv'\n",
    "    udf = pd.read_csv(csv_file, dtype=uber_col_type, parse_dates=['key', 'pickup_datetime'])\n",
    "\n",
    "    # Connect to the SQLite database\n",
    "    engine = create_engine('sqlite:///part2.sqlite3')\n",
    "\n",
    "    # Write the DataFrame to an SQLite table\n",
    "    table_name = 'uber'\n",
    "    udf.to_sql(table_name, engine, if_exists='replace', index=False)\n",
    "\n",
    "    engine.dispose()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call create_yellow_taxi() to create table for yellow taxi\n",
    "create_yellow_taxi()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call create_uber() to create table for yellow taxi\n",
    "create_uber()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_weather_hourly() -> None:\n",
    "  \"\"\"Create a SQLite database, create and populate a table for sampled dataset of hourly weather information\"\"\"\n",
    "  # Read weather data\n",
    "  string = str\n",
    "  f32 = np.float32\n",
    "  weather_col_type = {'STATION': int, 'LATITUDE': f32, 'LONGITUDE': f32, \n",
    "                      'ELEVATION': f32, 'NAME': string, 'REPORT_TYPE': string, 'SOURCE': str, \n",
    "                      'HourlyAltimeterSetting': string, 'HourlyDewPointTemperature': string,\n",
    "                      'HourlyDryBulbTemperature': string, 'HourlyPrecipitation': string,\n",
    "                      'HourlyPresentWeatherType': string, 'HourlyPressureChange': string,\n",
    "                      'HourlyPressureTendency': f32, 'HourlyRelativeHumidity': f32,\n",
    "                      'HourlySkyConditions': string, 'HourlySeaLevelPressure': string,\n",
    "                      'HourlyStationPressure': string, 'HourlyVisibility': string, \n",
    "                      'HourlyWetBulbTemperature': f32,\n",
    "                      'HourlyWindDirection': string, 'HourlyWindGustSpeed': f32,\n",
    "                      'HourlyWindSpeed': f32,\n",
    "                      'DailyWeather': string, 'DailySustainedWindDirection': f32,\n",
    "                      'DailySnowfall': string, 'DailySnowDepth': string, 'DailyPrecipitation': string,\n",
    "                      'DailyPeakWindSpeed': f32, 'DailyPeakWindDirection': f32,\n",
    "                      'DailyMinimumDryBulbTemperature': f32, 'DailyMaximumDryBulbTemperature': f32,\n",
    "                      'DailyHeatingDegreeDays': f32, 'DailyDepartureFromNormalAverageTemperature': f32,\n",
    "                      'DailyCoolingDegreeDays': f32, 'DailyAverageWindSpeed': f32,\n",
    "                      'DailyAverageWetBulbTemperature': f32, 'DailyAverageStationPressure': f32,\n",
    "                      'DailyAverageSeaLevelPressure': f32, 'DailyAverageRelativeHumidity': f32,\n",
    "                      'DailyAverageDryBulbTemperature': f32, 'DailyAverageDewPointTemperature': f32\n",
    "                      }\n",
    "\n",
    "  # Read all the CSV file into one pandas DataFrame\n",
    "  wdfs = []                     \n",
    "  for y in [2009, 2010, 2011, 2012, 2013, 2014, 2015]:\n",
    "    wdf = pd.read_csv(f'{y}_weather.csv', parse_dates=['DATE'], dtype=weather_col_type, low_memory=False)\n",
    "    wdfs.append(wdf)\n",
    "\n",
    "  wdf_all = pd.concat(wdfs)\n",
    "\n",
    "  # List out column names for hourly data\n",
    "  hourly = [\n",
    "      'STATION',\n",
    "      'DATE',\n",
    "      'LATITUDE',\n",
    "      'LONGITUDE',\n",
    "      'ELEVATION',\n",
    "      'NAME',\n",
    "      'REPORT_TYPE',\n",
    "      'SOURCE',\n",
    "      'HourlyAltimeterSetting',\n",
    "      'HourlyDewPointTemperature',\n",
    "      'HourlyDryBulbTemperature',\n",
    "      'HourlyPrecipitation',\n",
    "      'HourlyPresentWeatherType',\n",
    "      'HourlyPressureChange',\n",
    "      'HourlyPressureTendency',\n",
    "      'HourlyRelativeHumidity',\n",
    "      'HourlySkyConditions',\n",
    "      'HourlySeaLevelPressure',\n",
    "      'HourlyStationPressure',\n",
    "      'HourlyVisibility',\n",
    "      'HourlyWetBulbTemperature',\n",
    "      'HourlyWindDirection',\n",
    "      'HourlyWindGustSpeed',\n",
    "      'HourlyWindSpeed',\n",
    "      ]\n",
    "\n",
    "  # List out column names for daily data\n",
    "  daily = [\n",
    "      'STATION',\n",
    "      'DATE',\n",
    "      'LATITUDE',\n",
    "      'LONGITUDE',\n",
    "      'ELEVATION',\n",
    "      'NAME',\n",
    "      'REPORT_TYPE',\n",
    "      'SOURCE',\n",
    "      'DailyAverageDewPointTemperature',\n",
    "      'DailyAverageDryBulbTemperature',\n",
    "      'DailyAverageRelativeHumidity',\n",
    "      'DailyAverageSeaLevelPressure',\n",
    "      'DailyAverageStationPressure',\n",
    "      'DailyAverageWetBulbTemperature',\n",
    "      'DailyAverageWindSpeed',\n",
    "      'DailyCoolingDegreeDays',\n",
    "      'DailyDepartureFromNormalAverageTemperature',\n",
    "      'DailyHeatingDegreeDays',\n",
    "      'DailyMaximumDryBulbTemperature',\n",
    "      'DailyMinimumDryBulbTemperature',\n",
    "      'DailyPeakWindDirection',\n",
    "      'DailyPeakWindSpeed',\n",
    "      'DailyPrecipitation',\n",
    "      'DailySnowDepth',\n",
    "      'DailySnowfall',\n",
    "      'DailySustainedWindDirection',\n",
    "      'DailySustainedWindSpeed',\n",
    "      'DailyWeather'\n",
    "  ]\n",
    "\n",
    "  # Connect to the SQLite database\n",
    "  engine = create_engine('sqlite:///part2.sqlite3')\n",
    "\n",
    "  # Write the DataFrame to an SQLite table\n",
    "  wdf_all[hourly].to_sql('hourly_weather', engine, if_exists='replace', index=False)\n",
    "  \n",
    "  engine.dispose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call create_weather_hourly() to create table for hourly weather information\n",
    "create_weather_hourly()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "def create_weather_daily() -> None:\n",
    "    \"\"\"Create a SQLite database, create and populate a table for sampled dataset of daily weather information\n",
    "    Calculated by applying aggregation function to hourly sql dataset\"\"\"\n",
    "    weather_daily_query = \"\"\"\n",
    "        SELECT STATION,\n",
    "        DATE(DATE) AS day,\n",
    "        LATITUDE,\n",
    "        LONGITUDE,\n",
    "        ELEVATION,\n",
    "        NAME,REPORT_TYPE,\n",
    "        SOURCE,\n",
    "        AVG(HourlyDewPointTemperature) AS DailyAverageDewPointTemperature,\n",
    "        AVG(HourlyDryBulbTemperature) AS DailyAverageDryBulbTemperature,\n",
    "        AVG(HourlyRelativeHumidity) AS DailyAverageRelativeHumidity,\n",
    "        AVG(HourlySeaLevelPressure) AS DailyAverageSeaLevelPressure,\n",
    "        AVG(CAST(HourlyStationPressure AS FLOAT)) AS DailyAverageStationPressure,\n",
    "        AVG(HourlyWetBulbTemperature) AS DailyAverageWetBulbTemperature,\n",
    "        AVG(HourlyWindSpeed) AS DailyAverageWindSpeed,\n",
    "        MAX(HourlyDewPointTemperature) AS DailyMaximumDryBulbTemperature,\n",
    "        MIN(HourlyDewPointTemperature) AS DailyMinimumDryBulbTemperature,\n",
    "        MAX(CAST(HourlyWindDirection AS FLOAT)) AS DailyPeakWindDirection,\n",
    "        MAX(HourlyWindSpeed) AS DailyPeakWindSpeed,\n",
    "        SUM(HourlyPrecipitation) AS DailyPrecipitation,\n",
    "        AVG(HourlyWindDirection) AS DailySustainedWindDirection,\n",
    "        AVG(HourlyWindSpeed) AS DailySustainedWindSpeed\n",
    "       \n",
    "        FROM hourly_weather\n",
    "        GROUP BY day\n",
    "        \"\"\"\n",
    "    connection = sqlite3.connect('part2.sqlite3')\n",
    "    weather_daily_df = pd.read_sql_query(weather_daily_query, connection)\n",
    "    engine = create_engine('sqlite:///part2.sqlite3')\n",
    "    weather_daily_df.to_sql('daily_weather', engine, if_exists='replace', index=False)\n",
    "    engine.dispose()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call create_weather_daily() to create table for daily weather information\n",
    "create_weather_daily()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create schema file\n",
    "create_table = \"\"\"\n",
    "CREATE TABLE IF NOT EXISTS daily_wealther (\n",
    "\t\"STATION\" BIGINT, \n",
    "\t\"DATE\" DATETIME, \n",
    "\t\"LATITUDE\" FLOAT, \n",
    "\t\"LONGITUDE\" FLOAT, \n",
    "\t\"ELEVATION\" FLOAT, \n",
    "\t\"NAME\" TEXT, \n",
    "\t\"REPORT_TYPE\" TEXT, \n",
    "\t\"SOURCE\" TEXT, \n",
    "\t\"DailyAverageDewPointTemperature\" FLOAT, \n",
    "\t\"DailyAverageDryBulbTemperature\" FLOAT, \n",
    "\t\"DailyAverageRelativeHumidity\" FLOAT, \n",
    "\t\"DailyAverageSeaLevelPressure\" FLOAT, \n",
    "\t\"DailyAverageStationPressure\" FLOAT, \n",
    "\t\"DailyAverageWetBulbTemperature\" FLOAT, \n",
    "\t\"DailyAverageWindSpeed\" FLOAT, \n",
    "\t\"DailyCoolingDegreeDays\" FLOAT, \n",
    "\t\"DailyDepartureFromNormalAverageTemperature\" FLOAT, \n",
    "\t\"DailyHeatingDegreeDays\" FLOAT, \n",
    "\t\"DailyMaximumDryBulbTemperature\" FLOAT, \n",
    "\t\"DailyMinimumDryBulbTemperature\" FLOAT, \n",
    "\t\"DailyPeakWindDirection\" FLOAT, \n",
    "\t\"DailyPeakWindSpeed\" FLOAT, \n",
    "\t\"DailyPrecipitation\" TEXT, \n",
    "\t\"DailySnowDepth\" TEXT, \n",
    "\t\"DailySnowfall\" TEXT, \n",
    "\t\"DailySustainedWindDirection\" FLOAT, \n",
    "\t\"DailySustainedWindSpeed\" FLOAT, \n",
    "\t\"DailyWeather\" TEXT\n",
    ")\n",
    "\n",
    ";\n",
    "\n",
    "\n",
    "\n",
    "CREATE TABLE IF NOT EXISTS hourly_wealther (\n",
    "\t\"STATION\" BIGINT, \n",
    "\t\"DATE\" DATETIME, \n",
    "\t\"LATITUDE\" FLOAT, \n",
    "\t\"LONGITUDE\" FLOAT, \n",
    "\t\"ELEVATION\" FLOAT, \n",
    "\t\"NAME\" TEXT, \n",
    "\t\"REPORT_TYPE\" TEXT, \n",
    "\t\"SOURCE\" TEXT, \n",
    "\t\"HourlyAltimeterSetting\" TEXT, \n",
    "\t\"HourlyDewPointTemperature\" TEXT, \n",
    "\t\"HourlyDryBulbTemperature\" TEXT, \n",
    "\t\"HourlyPrecipitation\" TEXT, \n",
    "\t\"HourlyPresentWeatherType\" TEXT, \n",
    "\t\"HourlyPressureChange\" TEXT, \n",
    "\t\"HourlyPressureTendency\" FLOAT, \n",
    "\t\"HourlyRelativeHumidity\" FLOAT, \n",
    "\t\"HourlySkyConditions\" TEXT, \n",
    "\t\"HourlySeaLevelPressure\" TEXT, \n",
    "\t\"HourlyStationPressure\" TEXT, \n",
    "\t\"HourlyVisibility\" TEXT, \n",
    "\t\"HourlyWetBulbTemperature\" FLOAT, \n",
    "\t\"HourlyWindDirection\" TEXT, \n",
    "\t\"HourlyWindGustSpeed\" FLOAT, \n",
    "\t\"HourlyWindSpeed\" FLOAT\n",
    ")\n",
    "\n",
    ";\n",
    "\n",
    "\n",
    "\n",
    "CREATE TABLE IF NOT EXISTS uber (\n",
    "\t\"Unnamed: 0.1\" BIGINT, \n",
    "\t\"Unnamed: 0\" BIGINT, \n",
    "\t\"key\" DATETIME, \n",
    "\tfare_amount FLOAT, \n",
    "\tpickup_datetime TIMESTAMP, \n",
    "\tpickup_longitude FLOAT, \n",
    "\tpickup_latitude FLOAT, \n",
    "\tdropoff_longitude FLOAT, \n",
    "\tdropoff_latitude FLOAT, \n",
    "\tpassenger_count BIGINT, \n",
    "\tdistance FLOAT\n",
    ")\n",
    "\n",
    ";\n",
    "\n",
    "\n",
    "\n",
    "CREATE TABLE IF NOT EXISTS yellow_taxi (\n",
    "\t\"Unnamed: 0\" BIGINT, \n",
    "\tvendor_id TEXT, \n",
    "\tpickup_datetime DATETIME, \n",
    "\tdropoff_datetime DATETIME, \n",
    "\tpassenger_count SMALLINT, \n",
    "\ttrip_distance FLOAT, \n",
    "\tpickup_longitude FLOAT, \n",
    "\tpickup_latitude FLOAT, \n",
    "\tdropoff_longitude FLOAT, \n",
    "\tdropoff_latitude FLOAT, \n",
    "\tpayment_type TEXT, \n",
    "\ttip_amount FLOAT, \n",
    "\ttolls_amount FLOAT, \n",
    "\ttotal_amount FLOAT, \n",
    "\tfare_amount FLOAT, \n",
    "\tdistance FLOAT\n",
    ")\n",
    ";\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "with open(\"schema.sql\", \"w\") as schema_file:\n",
    "    schema_file.write(create_table)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part3: Understanding Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sqlalchemy import create_engine\n",
    "connection = sqlite3.connect('part2.sqlite3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query1\n",
    "# Count number of times yellow taxi was used from 01-2009 to 06-2015 for each hour of the day in descending order\n",
    "\n",
    "def part3_q1() -> None:\n",
    "    q1_sql = \"\"\"\n",
    "    SELECT COUNT(vendor_id) AS count, strftime('%H', pickup_datetime) hour \n",
    "    FROM yellow_taxi\n",
    "    GROUP BY strftime('%H', pickup_datetime)\n",
    "    ORDER BY count DESC\n",
    "    \"\"\"\n",
    "    # excute the query\n",
    "    with connection:\n",
    "        result = connection.execute(q1_sql)\n",
    "\n",
    "    # save query\n",
    "    with open(\"yellow_taxi_popularity_in_each_hour_of_day.sql\", \"w\") as query1:\n",
    "        query1.write(q1_sql)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "part3_q1()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query2\n",
    "# Count number of times uber was used from 01-2009 to 06-2015 for each day of the week in descending order\n",
    "\n",
    "def part2_q2() -> None:\n",
    "    q2_sql = \"\"\"\n",
    "    SELECT COUNT(key) AS count, strftime('%w', pickup_datetime) hour \n",
    "    FROM uber\n",
    "    GROUP BY strftime('%w', pickup_datetime)\n",
    "    ORDER BY count DESC\n",
    "    \"\"\"\n",
    "    # excute the query\n",
    "    with connection:\n",
    "        result = connection.execute(q2_sql)\n",
    "\n",
    "    # save query\n",
    "    with open(\"uber_popularity_in_each_day_of_week.sql\", \"w\") as query2:\n",
    "        query2.write(q2_sql)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "part2_q2()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# query3\n",
    "# Count 95% percentile of distance traveled for all hired trips during July 2013\n",
    "\n",
    "def part3_q3() -> None:\n",
    "    q3_sql = \"\"\"\n",
    "    WITH taxi_distance AS (\n",
    "        SELECT trip_distance \n",
    "        FROM yellow_taxi\n",
    "        WHERE pickup_datetime >= '2013-07-01' AND pickup_datetime < '2013-08-01'\n",
    "    ), \n",
    "    uber_distance AS (\n",
    "        SELECT distance \n",
    "        FROM uber\n",
    "        WHERE pickup_datetime >= '2013-07-01' AND pickup_datetime < '2013-08-01'\n",
    "    ) \n",
    "\n",
    "    SELECT trip_distance\n",
    "    FROM (\n",
    "        SELECT trip_distance FROM taxi_distance \n",
    "        UNION ALL \n",
    "        SELECT distance FROM uber_distance\n",
    "    ) AS all_distances\n",
    "    ORDER BY trip_distance  DESC\n",
    "    LIMIT 1\n",
    "    OFFSET ROUND((SELECT COUNT(*) FROM (\n",
    "                      SELECT trip_distance  FROM taxi_distance \n",
    "                      UNION ALL \n",
    "                      SELECT distance  FROM uber_distance\n",
    "                                        )\n",
    "                 ) * 0.95) - 1\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    # excute the query\n",
    "    with connection:\n",
    "        result = connection.execute(q3_sql)\n",
    "\n",
    "    # save query\n",
    "    with open(\"hired_trips_95%_percentile_in_July2013.sql\", \"w\") as query3:\n",
    "        query3.write(q3_sql)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "part3_q3()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# query 4\n",
    "# Count the top 10 days with the highest number of hired rides for 2009 and the average distance for each day\n",
    "\n",
    "def part3_q4() -> None:\n",
    "    q4_sql = \"\"\"\n",
    "    WITH hired_rides AS (\n",
    "        SELECT pickup_datetime,\n",
    "               COUNT(*) num_rides,\n",
    "               AVG(trip_distance) avg_distance\n",
    "        FROM (\n",
    "            SELECT pickup_datetime, trip_distance\n",
    "            FROM yellow_taxi\n",
    "            WHERE pickup_datetime >= '2009-01-01' AND pickup_datetime < '2010-01-01'\n",
    "            UNION ALL\n",
    "            SELECT pickup_datetime, distance\n",
    "            FROM uber\n",
    "            WHERE pickup_datetime >= '2009-01-01' AND pickup_datetime < '2010-01-01'\n",
    "             ) AS all_rides\n",
    "        GROUP BY strftime('%j', pickup_datetime)\n",
    "        ORDER BY num_rides DESC\n",
    "        LIMIT 10\n",
    "    )\n",
    "\n",
    "    SELECT pickup_datetime, num_rides, avg_distance\n",
    "    FROM hired_rides\n",
    "    \"\"\"\n",
    "\n",
    "    # save query\n",
    "    with open(\"top10_days_with_highest_hired_rides_in_2009.sql\", \"w\") as query4:\n",
    "        query4.write(q4_sql)\n",
    "        \n",
    "    # excute the query\n",
    "    with connection:\n",
    "        result = connection.execute(q4_sql)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "part3_q4()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# query5\n",
    "# Count top 10 windiest days on average in 2014 and number of hired trips on those days\n",
    "\n",
    "def part3_q5() -> None:\n",
    "    q5_sql = \"\"\"\n",
    "    WITH hired_rides AS (\n",
    "        SELECT strftime('%j', pickup_datetime) date,\n",
    "               COUNT(*) num_rides\n",
    "        FROM (\n",
    "            SELECT pickup_datetime\n",
    "            FROM yellow_taxi\n",
    "            WHERE pickup_datetime >= '2014-01-01' AND pickup_datetime < '2015-01-01'\n",
    "            UNION ALL\n",
    "            SELECT pickup_datetime\n",
    "            FROM uber\n",
    "            WHERE pickup_datetime >= '2014-01-01' AND pickup_datetime < '2015-01-01'\n",
    "             ) AS all_rides\n",
    "        GROUP BY strftime('%j', pickup_datetime)\n",
    "    )\n",
    "\n",
    "    SELECT day, DailyAverageWindSpeed, num_rides\n",
    "    FROM hired_rides \n",
    "    JOIN daily_weather ON hired_rides.date = strftime('%j', daily_weather.day)\n",
    "    WHERE day >= '2014-01-01' AND day < '2015-01-01'\n",
    "    ORDER BY DailyAverageWindSpeed DESC\n",
    "    LIMIT 10\n",
    "    \"\"\"\n",
    "\n",
    "    # excute the query\n",
    "    with connection:\n",
    "        result = connection.execute(q5_sql)\n",
    "\n",
    "    # save query\n",
    "    with open(\"top10_windiest_days_in_2014.sql\", \"w\") as query5:\n",
    "        query5.write(q5_sql)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "part3_q5()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# query6\n",
    "# Count number of trips taken, precipitation NYC received and sustained wind speed in each hour during\n",
    "# Hurricane Sandy in NYC (Oct 29-30, 2012), plus the week leading up and the week after\n",
    "\n",
    "def part3_q6()  -> None:\n",
    "    q6_sql = \"\"\"\n",
    "    WITH hired_rides AS (\n",
    "        SELECT strftime('%H', pickup_datetime) pick_day,\n",
    "               COUNT(*) num_rides\n",
    "        FROM (\n",
    "            SELECT pickup_datetime\n",
    "            FROM yellow_taxi\n",
    "            WHERE pickup_datetime >= '2012-10-22' AND pickup_datetime < '2012-11-06'\n",
    "            UNION ALL\n",
    "            SELECT pickup_datetime\n",
    "            FROM uber\n",
    "            WHERE pickup_datetime >= '2012-10-22' AND pickup_datetime < '2012-11-06'\n",
    "             ) AS all_rides\n",
    "        GROUP BY strftime('%H', pickup_datetime)\n",
    "    )\n",
    "\n",
    "    SELECT DATE, num_rides, HourlyPrecipitation, HourlyWindSpeed\n",
    "    FROM hired_rides \n",
    "    JOIN hourly_weather ON hired_rides.pick_day = strftime('%H', hourly_weather.DATE)\n",
    "    WHERE DATE >= '2012-10-22' AND DATE < '2012-11-06'\n",
    "    ORDER BY DATE \n",
    "    \"\"\"\n",
    "    # excute the query\n",
    "    with connection:\n",
    "        result = connection.execute(q6_sql)\n",
    "\n",
    "    # save query\n",
    "    with open(\"trips_during_HurricaneSand_in_NYC.sql\", \"w\") as query6:\n",
    "        query6.write(q6_sql)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "part3_q6()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Visualizing the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization 1\n",
    "def visual_1() -> None:\n",
    "    \"\"\"Create a bar chart visualization for the popularity of Yellow Taxi rides for each hour of the day\"\"\"\n",
    "    query1_df = pd.read_sql_query(q1_sql, connection)\n",
    "    fig = px.bar(query1_df, x='hour', y='count', template = \"presentation\")\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call function for visualization 1\n",
    "visual_1()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualiztion 3\n",
    "def visual_3() -> None:\n",
    "    \"\"\"Create a bar chart visualization that compares what day of the week was most popular for drop offs for each airport\n",
    "    Airports included: JFK, LGA, EWR, with each lat/long coordinate boxes defined in the function\n",
    "    Graph lable: \n",
    "    weekday: 0 = Sunday, 1 = Monday, 2 = Tuesday, 3 = Wednesday, 4 = Thursday, 5 = Friday, 6 = Saturday\n",
    "    \"\"\"\n",
    "    # Define three lat/long coordinate boxes around the three major New York airports\n",
    "    JFK_box = (min_longitude, max_longitude, min_latitude, max_latitude) = (-73.823574, -73.746337, \n",
    "                                                                            40.621097, 40.668794)\n",
    "    LGA_box = (min_longitude, max_longitude, min_latitude, max_latitude) = (-73.890339,-73.854038,\n",
    "                                                                            40.764589, 40.787675)\n",
    "    EWR_box = (min_longitude, max_longitude, min_latitude, max_latitude) = (-74.235392, -74.149495,\n",
    "                                                                            40.665601, 40.711991)\n",
    "    query_v3 = \"\"\"\n",
    "    WITH temp_table AS(\n",
    "    WITH uber_coor AS(\n",
    "    SELECT CASE WHEN dropoff_latitude < 40.668794 AND dropoff_latitude > 40.621097\n",
    "                AND dropoff_longitude < -73.746337 AND dropoff_longitude > -73.823574 THEN \"JFK\"\n",
    "                WHEN dropoff_latitude < 40.787675  AND dropoff_latitude > 40.764589\n",
    "                AND dropoff_longitude < -73.854038 AND dropoff_longitude > -73.890339 THEN \"LGA\"\n",
    "                WHEN dropoff_latitude < 40.711991 AND dropoff_latitude > 40.665601\n",
    "                AND dropoff_longitude < -74.149495 AND dropoff_longitude > -74.235392 THEN \"EWR\"\n",
    "                ELSE \"NONE\" END AS airports, \n",
    "                strftime('%w', pickup_datetime) as weekday\n",
    "    FROM uber\n",
    "    ),\n",
    "    yellow_coor AS(\n",
    "    SELECT CASE WHEN dropoff_latitude < 40.668794 AND dropoff_latitude > 40.621097\n",
    "                AND dropoff_longitude < -73.746337 AND dropoff_longitude > -73.823574 THEN \"JFK\"\n",
    "                WHEN dropoff_latitude < 40.787675  AND dropoff_latitude > 40.764589\n",
    "                AND dropoff_longitude < -73.854038 AND dropoff_longitude > -73.890339 THEN \"LGA\"\n",
    "                WHEN dropoff_latitude < 40.711991 AND dropoff_latitude > 40.665601\n",
    "                AND dropoff_longitude < -74.149495 AND dropoff_longitude > -74.235392 THEN \"EWR\"\n",
    "                ELSE \"NONE\" END AS airports, \n",
    "                strftime('%w', pickup_datetime) as weekday\n",
    "    FROM yellow_taxi\n",
    "    )\n",
    "    SELECT count(*) as count, airports, weekday \n",
    "    FROM yellow_coor\n",
    "    WHERE airports != \"NONE\"\n",
    "    GROUP BY airports, weekday\n",
    "    UNION ALL\n",
    "    SELECT count(*) as count, airports, weekday\n",
    "    FROM uber_coor\n",
    "    WHERE airports != \"NONE\"\n",
    "    GROUP BY airports, weekday\n",
    "    )\n",
    "    SELECT SUM(count) as count, airports, weekday\n",
    "    FROM temp_table\n",
    "    GROUP BY airports, weekday\n",
    "    \"\"\"\n",
    "    query3_df = pd.read_sql_query(query_v3, connection)\n",
    "    fig = px.bar(query3_df, x=\"airports\", y=\"count\",\n",
    "             color=\"weekday\",\n",
    "             barmode = 'group')\n",
    "                  \n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call function for visualization 3\n",
    "visual_3()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualiztion 4\n",
    "def visual_4() -> None:\n",
    "    \"\"\"Create a heatmap visualization of all hired trips over a map of the area\"\"\"\n",
    "    query_v4 = \"\"\"\n",
    "    WITH uber_coor AS(\n",
    "    SELECT pickup_longitude, pickup_latitude\n",
    "    FROM uber\n",
    "    ),\n",
    "    yellow_coor AS(\n",
    "    SELECT pickup_longitude, pickup_latitude\n",
    "    FROM yellow_taxi\n",
    "    )\n",
    "    SELECT count(*), pickup_longitude, pickup_latitude \n",
    "    FROM yellow_coor\n",
    "    GROUP BY pickup_longitude, pickup_latitude\n",
    "    UNION ALL\n",
    "    SELECT count(*), pickup_longitude, pickup_latitude \n",
    "    FROM uber_coor\n",
    "    GROUP BY pickup_longitude, pickup_latitude\n",
    "    \"\"\"\n",
    "    query4_df = pd.read_sql_query(query_v4, connection)\n",
    "    fig = px.density_mapbox(query4_df, lat='pickup_latitude', lon='pickup_longitude', z='count(*)',\n",
    "                        mapbox_style=\"stamen-terrain\")\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call function for visualization 4\n",
    "visual_4()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization 5\n",
    "def visual_5() -> None:\n",
    "    \"\"\"Create a scatter plot that compares tip amount versus distance for Yellow Taxi rides\"\"\"\n",
    "    query_v5 = \"\"\"\n",
    "    SELECT trip_distance, tip_amount\n",
    "    FROM yellow_taxi\n",
    "    \"\"\"\n",
    "    query5_df = pd.read_sql_query(query_v5, connection)\n",
    "    # filter for outliers\n",
    "    # StackOverflow: https://stackoverflow.com/questions/23199796/detect-and-exclude-outliers-in-a-pandas-dataframe\n",
    "    # Author: user6903745\n",
    "    query5_df = query5_df[query5_df[\"tip_amount\"] < query5_df[\"tip_amount\"].quantile(0.99999999)]\n",
    "    query5_df = query5_df[query5_df[\"trip_distance\"] < query5_df[\"trip_distance\"].quantile(0.9999)]\n",
    "    # from scipy import stats\n",
    "    # query5_df = query5_df[(np.abs(stats.zscore(query5_df)) < 30).all(axis=1)]\n",
    "    fig = px.scatter(query5_df, x=\"trip_distance\", y=\"tip_amount\", template='presentation')\n",
    "    fig.show()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call function for visualization 5\n",
    "visual_5()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization 6\n",
    "def visual_6() -> None:\n",
    "    \"\"\"Create a scatter plot that compares tip amount versus precipitation amount for Yellow Taxi rides\"\"\"\n",
    "    query_v6 = \"\"\"\n",
    "    with yt as (\n",
    "        SELECT tip_amount, DATE(pickup_datetime) as yt_day\n",
    "        FROM yellow_taxi as yt)\n",
    "    SELECT tip_amount, DailyPrecipitation\n",
    "    FROM yt\n",
    "    LEFT JOIN daily_weather as dw\n",
    "    ON yt.yt_day = day\n",
    "    \"\"\"\n",
    "    query6_df = pd.read_sql_query(query_v6, connection)\n",
    "    fig = px.scatter(query6_df, x=\"DailyPrecipitation\", y=\"tip_amount\", template='presentation')\n",
    "    fig.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call function for visualization 6\n",
    "visual_6()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
