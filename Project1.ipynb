{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Project Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all import statements needed for the project\n",
    "import os \n",
    "import requests\n",
    "import re \n",
    "import time \n",
    "import urllib.request\n",
    "from bs4 import BeautifulSoup\n",
    "from joblib import Parallel, delayed\n",
    "import math \n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "import geopandas as gpd\n",
    "import pyarrow.parquet as pq"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import requests\n",
    "import re \n",
    "import time \n",
    "import urllib.request\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def url_date_ge(a: str, b: str) -> bool:\n",
    "  \"\"\"Return a boolean that tells whether the input date a is greater than or equal to date b\"\"\"\n",
    "  return (int(a[:4]), int(a[-2:])) >= (int(b[:4]), int(b[-2:]))\n",
    "\n",
    "def url_date_le(a: str, b: str) -> bool:\n",
    "  \"\"\"Return a boolean that tells whether the input date a is less than or equal to date b\"\"\"\n",
    "  return (int(a[:4]), int(a[-2:])) <= (int(b[:4]), int(b[-2:]))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List out url that will be using\n",
    "uber_url = 'https://drive.google.com/file/d/1F7D82w1D5151GXCR6BTEk7mNQ_YnPNDk/view?usp=sharing'\n",
    "weather_url = 'https://drive.google.com/drive/folders/1I_Cj3RFHRGcQjb5Gas06buqRbKodIwKC?usp=sharing'\n",
    "taxi_url = 'https://www.nyc.gov/site/tlc/about/tlc-trip-record-data.page'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_yellow_taxi_parquet(taxi_url: str, \n",
    "                                 from_date: str = '2009-01', to_date: str = '2015-06',\n",
    "                                 taxi_par_save_dir: str = 'yellow_taxi',\n",
    "                                 max_waiting_time: float = 10 * 60)  -> None:\n",
    "  \"\"\"Download the yellow taxi parquet files from taxi_url.\"\"\"\n",
    "  # Make a request to the webpage\n",
    "  response = requests.get(taxi_url)\n",
    "  # Use BeautifulSoup to parse the HTML content of the webpage\n",
    "  soup = BeautifulSoup(response.content, 'html.parser')\n",
    "  data_urls = [tag['href'] for tag in soup.find_all(attrs={'title': 'Yellow Taxi Trip Records'})]\n",
    "  inrange_urls = [_url for _url in data_urls \n",
    "                if url_date_ge(re.search(r'(\\d+-\\d+)', _url).group(0), from_date) \n",
    "                and url_date_le(re.search(r'(\\d+-\\d+)', _url).group(0), to_date)\n",
    "                ]\n",
    "  \n",
    "  # Create folder\n",
    "  if not os.path.exists(taxi_par_save_dir):\n",
    "    os.mkdir(taxi_par_save_dir)\n",
    "\n",
    "  done = False\n",
    "  st = time.time()\n",
    "  while not done and time.time() - st < max_waiting_time:\n",
    "    try:\n",
    "      for _url in inrange_urls:\n",
    "        save_file = os.path.join(taxi_par_save_dir, _url[_url.index('yellow'):])\n",
    "        if not os.path.exists(save_file):\n",
    "          print(f'downloading to {save_file}...')\n",
    "          urllib.request.urlretrieve(_url, save_file)\n",
    "          print('finished.')\n",
    "        else:\n",
    "          print(f'{save_file} already downloaded.')\n",
    "      \n",
    "      done = True\n",
    "      print('All downloads finished.')\n",
    "    except:\n",
    "      continue \n",
    "  \n",
    "  if not done:\n",
    "    print('Not all files downloaded. Might be insufficient max waiting time. You may re-run this function.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute function to download yellow taxi parquet files\n",
    "download_yellow_taxi_parquet(taxi_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_unzip_shapefile(taxi_url: str, shapefile_save_dir: str = 'assets') -> None:\n",
    "  \"\"\"Download and unzip the Taxi Zone Shapefile from taxi_url.\"\"\"\n",
    "  # Make a request to the webpage\n",
    "  response = requests.get(taxi_url)\n",
    "  # Use BeautifulSoup to parse the HTML content of the webpage\n",
    "  soup = BeautifulSoup(response.content, 'html.parser')\n",
    "  shapefile_url = soup.find(string='Taxi Zone Shapefile').parent['href']\n",
    "\n",
    "  if not os.path.exists(shapefile_save_dir):\n",
    "    os.mkdir(shapefile_save_dir)\n",
    "  save_file = os.path.join(shapefile_save_dir, 'taxi_zones.zip')\n",
    "  if not os.path.exists(save_file):\n",
    "    urllib.request.urlretrieve(shapefile_url, save_file)\n",
    "  if not os.path.exists(os.path.join('.', 'taxi_zones.shp')):\n",
    "    !unzip {save_file} \n",
    "  print('Downloaded shapefile.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute the function to download and unzip the Taxi Zone Shapefile from taxi_url\n",
    "download_unzip_shapefile(taxi_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "from joblib import Parallel, delayed\n",
    "import math \n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "import geopandas as gpd\n",
    "import pyarrow.parquet as pq\n",
    "\n",
    "class Part1:\n",
    "  def __init__(self, \n",
    "               taxi_par_save_dir='yellow_taxi',\n",
    "               uber_path='uber_rides_sample.csv',\n",
    "               shapefile_save_dir='.',\n",
    "               processed_path='processed'):\n",
    "    self.taxi_par_save_dir = taxi_par_save_dir\n",
    "    self.shapefile_save_dir = shapefile_save_dir\n",
    "    self.shp_path = os.path.join(shapefile_save_dir, 'taxi_zones.shp')\n",
    "    self.uber_path = uber_path\n",
    "    self.processed_path = processed_path\n",
    "    gdf = gpd.read_file(self.shp_path)\n",
    "    gdf['centroid'] = gdf.geometry.centroid.to_crs(4326)\n",
    "    gdf = gdf.set_index('OBJECTID')\n",
    "    self.gdf = gdf\n",
    "    self.pars = os.listdir(taxi_par_save_dir)\n",
    "    self.pars = sorted(self.pars, key=lambda x: (int(x[x.index('.')-7: x.index('.')-3]), \n",
    "                                            int(x[x.index('.')-2: x.index('.')])))\n",
    "    self.box = (min_longitude, max_longitude, min_latitude, max_latitude) = (-74.242330, -73.717047, \n",
    "                                                                             40.560445, 40.908524)\n",
    "    \n",
    "    # cols\n",
    "    self.cols_mapping = {'vendor_name': 'vendor_id',\n",
    "                        'VendorID': 'vendor_id',\n",
    "                        'tpep_pickup_datetime': 'pickup_datetime',\n",
    "                        'Trip_Pickup_DateTime': 'pickup_datetime',\n",
    "                        'tpep_dropoff_datetime': 'dropoff_datetime',\n",
    "                        'Trip_Dropoff_DateTime': 'dropoff_datetime',\n",
    "                        'Passenger_Count': 'passenger_count',\n",
    "                        'Trip_Distance': 'trip_distance',\n",
    "                        'Start_Lon': 'pickup_longitude',\n",
    "                        'Start_Lat': 'pickup_latitude',\n",
    "                        'End_Lon': 'dropoff_longitude', \n",
    "                        'End_Lat': 'dropoff_latitude',\n",
    "                        'Payment_Type': 'payment_type',\n",
    "                        'Tip_Amt': 'tip_amount',\n",
    "                        'Tolls_Amt': 'tolls_amount',\n",
    "                        'Total_Amt': 'total_amount',\n",
    "                        'Fare_Amt': 'fare_amount'\n",
    "                        }\n",
    "    self.cols = ['vendor_id', 'pickup_datetime', 'dropoff_datetime', 'passenger_count',\n",
    "            'trip_distance', 'pickup_longitude', 'pickup_latitude', 'dropoff_longitude',\n",
    "            'dropoff_latitude', 'payment_type', 'tip_amount', 'tolls_amount', \n",
    "            'total_amount', 'fare_amount']\n",
    "\n",
    "    self.types = {'vendor_id': str, 'pickup_datetime': \"datetime64[ns]\", 'dropoff_datetime': \"datetime64[ns]\", \n",
    "            'passenger_count': np.int8, 'trip_distance': np.float32, 'pickup_longitude': np.float32,\n",
    "            'pickup_latitude': np.float32, 'dropoff_longitude': np.float32, 'dropoff_latitude': np.float32,\n",
    "            'payment_type': str, 'tip_amount': np.float32, 'tolls_amount': np.float32, \n",
    "            'total_amount': np.float32, 'fare_amount': np.float32}        \n",
    "  \n",
    "  def process_yellow_taxi_parquets(self)-> pd.DataFrame:\n",
    "    \"\"\"Reture a dataframe of all processed yellow taxi parquet files\"\"\"\n",
    "    if not os.path.exists(self.processed_path):\n",
    "      os.mkdir(self.processed_path)\n",
    "    clean_sampled_df_list = [0] * len(self.pars)\n",
    "    sample_ratio = self._determine_sample_ratio()\n",
    "    for i, par in enumerate(self.pars):\n",
    "      clean_sampled_df = self._process_one_yellow_taxi_parquet(par)\n",
    "      clean_sampled_df_list[i] = clean_sampled_df\n",
    "    \n",
    "    processed_all_yellow_taxi_df = pd.concat(clean_sampled_df_list)\n",
    "    return processed_all_yellow_taxi_df\n",
    "  \n",
    "  def _determine_sample_ratio(self) -> int:\n",
    "    \"\"\"Return a ratio, used for sampling yellow taxi data\"\"\"\n",
    "    uber_df = pd.read_csv(self.uber_path)\n",
    "    uber_rows = len(uber_df)\n",
    "    num_rows = 0\n",
    "    for i in range(len(self.pars)):\n",
    "      data_path = os.path.join(self.taxi_par_save_dir, self.pars[i])\n",
    "      pf = pq.ParquetFile(data_path)\n",
    "      # count the number of rows in the file\n",
    "      num_rows += pf.metadata.num_rows\n",
    "    self.sample_ratio = uber_rows / num_rows\n",
    "    return self.sample_ratio\n",
    "  \n",
    "  def _process_one_yellow_taxi_parquet(self, parquet_file: str) -> pd.DataFrame:\n",
    "    \"\"\"Return a processed dataset of the input parquet file\n",
    "    \n",
    "    Parameter:\n",
    "    parquet_file: one single yellow taxi parquet file\n",
    "    \"\"\"\n",
    "    data_path = os.path.join(self.taxi_par_save_dir, parquet_file)\n",
    "    large_parquet = pq.ParquetFile(data_path)\n",
    "    \n",
    "    prod_list = []\n",
    "    for trips in large_parquet.iter_batches(batch_size=100000):\n",
    "      trips = trips.to_pandas()\n",
    "      # Rename the columns by predefined column name in cols_mapping\n",
    "      prod_trips = trips.rename(columns={k: v for k, v in self.cols_mapping.items() if k in trips.columns})\n",
    "      # Lookup for latitude and longitude and add to dataframe\n",
    "      if 'DOLocationID' in trips.columns:\n",
    "        prod_trips = self.lookup_longlat('D', self.gdf, prod_trips)\n",
    "      if 'PULocationID' in trips.columns:\n",
    "        prod_trips = self.lookup_longlat('P', self.gdf, prod_trips)\n",
    "      \n",
    "      prod_trips = prod_trips[self.cols].astype(self.types).copy() # only a subset of columns are needed\n",
    "      prod_trips = prod_trips.dropna(axis=0, how='any', subset=['dropoff_longitude', 'dropoff_latitude', 'pickup_longitude', 'pickup_latitude'])\n",
    "      to_drop1 = prod_trips.apply(lambda row: not self._if_inside_box(row['dropoff_longitude'], row['dropoff_latitude']), axis=1)\n",
    "      to_drop2 = prod_trips.apply(lambda row: not self._if_inside_box(row['pickup_longitude'], row['pickup_latitude']), axis=1)\n",
    "      prod_trips = prod_trips.drop(prod_trips[to_drop1 + to_drop2].index)\n",
    "      prod_trips['trip_distance'] = prod_trips.apply(lambda row: \n",
    "                                                    self.calc_distance(row['pickup_longitude'], row['pickup_latitude'], row['dropoff_longitude'], row['dropoff_latitude']) \n",
    "                                                    if np.isnan(row['trip_distance']) \n",
    "                                                    else row['trip_distance'],\n",
    "                                                    axis=1)\n",
    "      prod_trips = prod_trips.sample(frac=self.sample_ratio)\n",
    "      prod_list.append(prod_trips)\n",
    "    prod_trips = pd.concat(prod_list)\n",
    "\n",
    "    par_savepath = os.path.join(self.processed_path, parquet_file)\n",
    "    if not os.path.exists(par_savepath):\n",
    "      prod_trips.to_parquet(par_savepath)\n",
    "    return prod_trips\n",
    "\n",
    "  def _if_inside_box(self, long: float, lat: float) -> bool:\n",
    "    \"\"\"Return a boolean that determines whether the trips start and/or end inside of the\n",
    "    latitude/longitude coordinate box: (40.560445, -74.242330) and (40.908524, -73.717047).\n",
    "\n",
    "    Parameter:\n",
    "    long: float, the longtitude of the trip\n",
    "    lat: float, the latitude of the trip\n",
    "    \"\"\"\n",
    "\n",
    "    min_longitude, max_longitude, min_latitude, max_latitude = self.box \n",
    "    return min_longitude <= long <= max_longitude and min_latitude <= lat <= max_latitude\n",
    "  \n",
    "  def lookup_longlat(self, loc_type: str, gdf: pd.DataFrame, prod_trips: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Return a processed dataframe for yellow taxi, \n",
    "    Add addtional column to record corresponding latitude and longitude, for location ID \n",
    "\n",
    "    Parameter:\n",
    "    loc_type: a string value, to indicate whether the input dataframe is prickup or dropoff, can be 'D' or 'P'\n",
    "    gdf: a dataframe of Taxi Zone Shapefile, one location ID corresponds to one set of latitude and longitude\n",
    "    prod_trips: a dataframe of yellow taxi location ID\n",
    "    \"\"\"\n",
    "\n",
    "    # Create column names according to input as pickup or dropoff\n",
    "    col_name = 'DOLocationID' if loc_type == 'D' else 'PULocationID'\n",
    "    add_name = 'dropoff' if loc_type == 'D' else 'pickup'\n",
    "    # Lookup for latitude and longitude for the location ID\n",
    "    found, lookup = self.lookup_with_id(prod_trips[col_name], gdf)\n",
    "    long, lat = found['centroid'].x, found['centroid'].y\n",
    "    # Create new empty columns\n",
    "    prod_trips[f'{add_name}_longitude'] = np.nan\n",
    "    prod_trips[f'{add_name}_latitude'] = np.nan\n",
    "    # Fill in the latitude and longitude value\n",
    "    prod_trips.loc[lookup.index, f'{add_name}_longitude'] = long.values\n",
    "    prod_trips.loc[lookup.index, f'{add_name}_latitude'] = lat.values\n",
    "    return prod_trips \n",
    "  \n",
    "  def lookup_with_id(self, sr: list, df: pd.DataFrame) -> tuple:\n",
    "    \"\"\"Look up for latitude and longitude of a given pickup and dropoff location ID\n",
    "    Return a tuple containing the set of latitude and longitude, \n",
    "    and the valid location ID from yellow taxi data that corresponds to it \n",
    "\n",
    "    Parameter:\n",
    "    sr: a series of yellow taxi pickup and dropoff location ID\n",
    "    df: a dataframe of Taxi Zone Shapefile, one location ID corresponds to one set of latitude and longitude   \n",
    "    \"\"\"\n",
    "    # filter for valid location ID in sr\n",
    "    lookup = sr[sr.isin(df.index)]\n",
    "    return df.loc[lookup], lookup\n",
    "  \n",
    "  def deg2rad(self, deg: float) -> float:\n",
    "    \"\"\"Convert a degree to radius\n",
    "    Parameter:\n",
    "    deg: float\n",
    "    \"\"\"\n",
    "    return deg * (math.pi/180)\n",
    "\n",
    "  def calc_distance(self, plong: float, plat: float, dlong: float, dlat: float) -> float:\n",
    "    \"\"\"Return a calculated distance in km between two latitude-longitude points\n",
    "    Parameter:\n",
    "    plong: float, pickup longitude\n",
    "    plat: float, pickup latitude\n",
    "    dlong: float, dropoff longitude\n",
    "    dlat: float, dropoff latitude\n",
    "    \"\"\"\n",
    "    # The following code was adapted from this StackOverflow \n",
    "    # answer from user1921: https://stackoverflow.com/questions/27928/calculate-distance-between-two-latitude-longitude-points-haversine-formula\n",
    "    R = 6371 # Radius of the earth in km\n",
    "    long = plong - dlong\n",
    "    lat = plat - dlat\n",
    "    lat_deg = self.deg2rad(lat)\n",
    "    long_deg = self.deg2rad(long)\n",
    "    a = (math.sin(lat_deg/2) * math.sin(lat_deg/2) \n",
    "    + math.cos(self.deg2rad(plat)) * math.cos(self.deg2rad(dlat)) \n",
    "    * math.sin(long_deg/2) * math.sin(long_deg/2)\n",
    "    )\n",
    "      \n",
    "    c = 2 * math.atan2(math.sqrt(a), math.sqrt(1-a)); \n",
    "    d = R * c # Distance in km\n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an instance of the defined class to process yellow taxi data\n",
    "p1 = Part1()\n",
    "processed_all_yellow_taxi_df = p1.process_yellow_taxi_parquets()\n",
    "# Save data to csv file\n",
    "processed_all_yellow_taxi_df.to_csv('processed_yellow_taxi.csv')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Storing Cleaned Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "def create_yellow_taxi() -> None:\n",
    "    \"\"\"Create a SQLite database, create and populate a table for sampled dataset of yellow taxi trips\"\"\"\n",
    "    yellow_taxi_col_type = {'vendor_id': str, \n",
    "                'passenger_count': np.int8, 'trip_distance': np.float32, 'pickup_longitude': np.float32,\n",
    "                'pickup_latitude': np.float32, 'dropoff_longitude': np.float32, 'dropoff_latitude': np.float32,\n",
    "                'payment_type': str, 'tip_amount': np.float32, 'tolls_amount': np.float32, \n",
    "                'total_amount': np.float32, 'fare_amount': np.float32} \n",
    "\n",
    "    # Read the CSV file into a pandas DataFrame\n",
    "    csv_file = 'processed_yellow_taxi.csv'\n",
    "    ydf = pd.read_csv(csv_file, dtype=yellow_taxi_col_type, parse_dates=['pickup_datetime', 'dropoff_datetime'])\n",
    "\n",
    "    # Connect to the SQLite database\n",
    "    engine = create_engine('sqlite:///part2.sqlite3')\n",
    "\n",
    "    # Write the DataFrame to an SQLite table\n",
    "    table_name = 'yellow_taxi'\n",
    "    ydf.to_sql(table_name, engine, if_exists='replace', index=False)\n",
    "\n",
    "    engine.dispose()\n",
    "\n",
    "def create_uber() -> None:\n",
    "    \"\"\"Create a SQLite database, create and populate a table for sampled dataset of uber trips\"\"\"\n",
    "    f32 = np.float32\n",
    "    uber_col_type = {\n",
    "        'fare_amount': f32, 'pickup_longitude': f32, 'pickup_latitude': f32, 'dropoff_longitude': f32, 'dropoff_latitude': f32,\n",
    "        'passenger_count': int, 'distance': f32\n",
    "    }            \n",
    "\n",
    "    # Read the CSV file into a pandas DataFrame\n",
    "    csv_file = 'processed_uber.csv'\n",
    "    udf = pd.read_csv(csv_file, dtype=uber_col_type, parse_dates=['key', 'pickup_datetime'])\n",
    "\n",
    "    # Connect to the SQLite database\n",
    "    engine = create_engine('sqlite:///part2.sqlite3')\n",
    "\n",
    "    # Write the DataFrame to an SQLite table\n",
    "    table_name = 'uber'\n",
    "    udf.to_sql(table_name, engine, if_exists='replace', index=False)\n",
    "\n",
    "    engine.dispose()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call create_yellow_taxi() to create table for yellow taxi\n",
    "create_yellow_taxi()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call create_uber() to create table for yellow taxi\n",
    "create_uber()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_weather_hourly() -> None:\n",
    "  \"\"\"Create a SQLite database, create and populate a table for sampled dataset of hourly weather information\"\"\"\n",
    "  # Read weather data\n",
    "  string = str\n",
    "  f32 = np.float32\n",
    "  weather_col_type = {'STATION': int, 'LATITUDE': f32, 'LONGITUDE': f32, \n",
    "                      'ELEVATION': f32, 'NAME': string, 'REPORT_TYPE': string, 'SOURCE': str, \n",
    "                      'HourlyAltimeterSetting': string, 'HourlyDewPointTemperature': string,\n",
    "                      'HourlyDryBulbTemperature': string, 'HourlyPrecipitation': string,\n",
    "                      'HourlyPresentWeatherType': string, 'HourlyPressureChange': string,\n",
    "                      'HourlyPressureTendency': f32, 'HourlyRelativeHumidity': f32,\n",
    "                      'HourlySkyConditions': string, 'HourlySeaLevelPressure': string,\n",
    "                      'HourlyStationPressure': string, 'HourlyVisibility': string, \n",
    "                      'HourlyWetBulbTemperature': f32,\n",
    "                      'HourlyWindDirection': string, 'HourlyWindGustSpeed': f32,\n",
    "                      'HourlyWindSpeed': f32,\n",
    "                      'DailyWeather': string, 'DailySustainedWindDirection': f32,\n",
    "                      'DailySnowfall': string, 'DailySnowDepth': string, 'DailyPrecipitation': string,\n",
    "                      'DailyPeakWindSpeed': f32, 'DailyPeakWindDirection': f32,\n",
    "                      'DailyMinimumDryBulbTemperature': f32, 'DailyMaximumDryBulbTemperature': f32,\n",
    "                      'DailyHeatingDegreeDays': f32, 'DailyDepartureFromNormalAverageTemperature': f32,\n",
    "                      'DailyCoolingDegreeDays': f32, 'DailyAverageWindSpeed': f32,\n",
    "                      'DailyAverageWetBulbTemperature': f32, 'DailyAverageStationPressure': f32,\n",
    "                      'DailyAverageSeaLevelPressure': f32, 'DailyAverageRelativeHumidity': f32,\n",
    "                      'DailyAverageDryBulbTemperature': f32, 'DailyAverageDewPointTemperature': f32\n",
    "                      }\n",
    "\n",
    "  # Read all the CSV file into one pandas DataFrame\n",
    "  wdfs = []                     \n",
    "  for y in [2009, 2010, 2011, 2012, 2013, 2014, 2015]:\n",
    "    wdf = pd.read_csv(f'{y}_weather.csv', parse_dates=['DATE'], dtype=weather_col_type, low_memory=False)\n",
    "    wdfs.append(wdf)\n",
    "\n",
    "  wdf_all = pd.concat(wdfs)\n",
    "\n",
    "  # List out column names for hourly data\n",
    "  hourly = [\n",
    "      'STATION',\n",
    "      'DATE',\n",
    "      'LATITUDE',\n",
    "      'LONGITUDE',\n",
    "      'ELEVATION',\n",
    "      'NAME',\n",
    "      'REPORT_TYPE',\n",
    "      'SOURCE',\n",
    "      'HourlyAltimeterSetting',\n",
    "      'HourlyDewPointTemperature',\n",
    "      'HourlyDryBulbTemperature',\n",
    "      'HourlyPrecipitation',\n",
    "      'HourlyPresentWeatherType',\n",
    "      'HourlyPressureChange',\n",
    "      'HourlyPressureTendency',\n",
    "      'HourlyRelativeHumidity',\n",
    "      'HourlySkyConditions',\n",
    "      'HourlySeaLevelPressure',\n",
    "      'HourlyStationPressure',\n",
    "      'HourlyVisibility',\n",
    "      'HourlyWetBulbTemperature',\n",
    "      'HourlyWindDirection',\n",
    "      'HourlyWindGustSpeed',\n",
    "      'HourlyWindSpeed',\n",
    "      ]\n",
    "\n",
    "  # List out column names for daily data\n",
    "  daily = [\n",
    "      'STATION',\n",
    "      'DATE',\n",
    "      'LATITUDE',\n",
    "      'LONGITUDE',\n",
    "      'ELEVATION',\n",
    "      'NAME',\n",
    "      'REPORT_TYPE',\n",
    "      'SOURCE',\n",
    "      'DailyAverageDewPointTemperature',\n",
    "      'DailyAverageDryBulbTemperature',\n",
    "      'DailyAverageRelativeHumidity',\n",
    "      'DailyAverageSeaLevelPressure',\n",
    "      'DailyAverageStationPressure',\n",
    "      'DailyAverageWetBulbTemperature',\n",
    "      'DailyAverageWindSpeed',\n",
    "      'DailyCoolingDegreeDays',\n",
    "      'DailyDepartureFromNormalAverageTemperature',\n",
    "      'DailyHeatingDegreeDays',\n",
    "      'DailyMaximumDryBulbTemperature',\n",
    "      'DailyMinimumDryBulbTemperature',\n",
    "      'DailyPeakWindDirection',\n",
    "      'DailyPeakWindSpeed',\n",
    "      'DailyPrecipitation',\n",
    "      'DailySnowDepth',\n",
    "      'DailySnowfall',\n",
    "      'DailySustainedWindDirection',\n",
    "      'DailySustainedWindSpeed',\n",
    "      'DailyWeather'\n",
    "  ]\n",
    "\n",
    "  # Connect to the SQLite database\n",
    "  engine = create_engine('sqlite:///part2.sqlite3')\n",
    "\n",
    "  # Write the DataFrame to an SQLite table\n",
    "  wdf_all[hourly].to_sql('hourly_weather', engine, if_exists='replace', index=False)\n",
    "  \n",
    "  engine.dispose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call create_weather() to create table for hourly and daily weather information\n",
    "create_weather_hourly()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "def create_weather_daily() -> None:\n",
    "    \"\"\"Create a SQLite database, create and populate a table for sampled dataset of daily weather information\n",
    "    Calculated by applying aggregation function to hourly sql dataset\"\"\"\n",
    "    weather_daily_query = \"\"\"\n",
    "        SELECT STATION,\n",
    "        DATE(DATE) AS day,\n",
    "        LATITUDE,\n",
    "        LONGITUDE,\n",
    "        ELEVATION,\n",
    "        NAME,REPORT_TYPE,\n",
    "        SOURCE,\n",
    "        AVG(HourlyDewPointTemperature) AS DailyAverageDewPointTemperature,\n",
    "        AVG(HourlyDryBulbTemperature) AS DailyAverageDryBulbTemperature,\n",
    "        AVG(HourlyRelativeHumidity) AS DailyAverageRelativeHumidity,\n",
    "        AVG(HourlySeaLevelPressure) AS DailyAverageSeaLevelPressure,\n",
    "        AVG(CAST(HourlyStationPressure AS FLOAT)) AS DailyAverageStationPressure,\n",
    "        AVG(HourlyWetBulbTemperature) AS DailyAverageWetBulbTemperature,\n",
    "        AVG(HourlyWindSpeed) AS DailyAverageWindSpeed,\n",
    "        MAX(HourlyDewPointTemperature) AS DailyMaximumDryBulbTemperature,\n",
    "        MIN(HourlyDewPointTemperature) AS DailyMinimumDryBulbTemperature,\n",
    "        MAX(CAST(HourlyWindDirection AS FLOAT)) AS DailyPeakWindDirection,\n",
    "        MAX(HourlyWindSpeed) AS DailyPeakWindSpeed,\n",
    "        SUM(HourlyPrecipitation) AS DailyPrecipitation,\n",
    "        AVG(HourlyWindDirection) AS DailySustainedWindDirection,\n",
    "        AVG(HourlyWindSpeed) AS DailySustainedWindSpeed\n",
    "       \n",
    "        FROM hourly_weather\n",
    "        GROUP BY day\n",
    "        \"\"\"\n",
    "    connection = sqlite3.connect('part2.sqlite3')\n",
    "    weather_daily_df = pd.read_sql_query(weather_daily_query, connection)\n",
    "    engine = create_engine('sqlite:///part2.sqlite3')\n",
    "    weather_daily_df.to_sql('daily_weather', engine, if_exists='replace', index=False)\n",
    "    engine.dispose()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_weather_daily()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create schema file\n",
    "create_table = \"\"\"\n",
    "CREATE TABLE IF NOT EXISTS daily_wealther (\n",
    "\t\"STATION\" BIGINT, \n",
    "\t\"DATE\" DATETIME, \n",
    "\t\"LATITUDE\" FLOAT, \n",
    "\t\"LONGITUDE\" FLOAT, \n",
    "\t\"ELEVATION\" FLOAT, \n",
    "\t\"NAME\" TEXT, \n",
    "\t\"REPORT_TYPE\" TEXT, \n",
    "\t\"SOURCE\" TEXT, \n",
    "\t\"DailyAverageDewPointTemperature\" FLOAT, \n",
    "\t\"DailyAverageDryBulbTemperature\" FLOAT, \n",
    "\t\"DailyAverageRelativeHumidity\" FLOAT, \n",
    "\t\"DailyAverageSeaLevelPressure\" FLOAT, \n",
    "\t\"DailyAverageStationPressure\" FLOAT, \n",
    "\t\"DailyAverageWetBulbTemperature\" FLOAT, \n",
    "\t\"DailyAverageWindSpeed\" FLOAT, \n",
    "\t\"DailyCoolingDegreeDays\" FLOAT, \n",
    "\t\"DailyDepartureFromNormalAverageTemperature\" FLOAT, \n",
    "\t\"DailyHeatingDegreeDays\" FLOAT, \n",
    "\t\"DailyMaximumDryBulbTemperature\" FLOAT, \n",
    "\t\"DailyMinimumDryBulbTemperature\" FLOAT, \n",
    "\t\"DailyPeakWindDirection\" FLOAT, \n",
    "\t\"DailyPeakWindSpeed\" FLOAT, \n",
    "\t\"DailyPrecipitation\" TEXT, \n",
    "\t\"DailySnowDepth\" TEXT, \n",
    "\t\"DailySnowfall\" TEXT, \n",
    "\t\"DailySustainedWindDirection\" FLOAT, \n",
    "\t\"DailySustainedWindSpeed\" FLOAT, \n",
    "\t\"DailyWeather\" TEXT\n",
    ")\n",
    "\n",
    ";\n",
    "\n",
    "\n",
    "\n",
    "CREATE TABLE IF NOT EXISTS hourly_wealther (\n",
    "\t\"STATION\" BIGINT, \n",
    "\t\"DATE\" DATETIME, \n",
    "\t\"LATITUDE\" FLOAT, \n",
    "\t\"LONGITUDE\" FLOAT, \n",
    "\t\"ELEVATION\" FLOAT, \n",
    "\t\"NAME\" TEXT, \n",
    "\t\"REPORT_TYPE\" TEXT, \n",
    "\t\"SOURCE\" TEXT, \n",
    "\t\"HourlyAltimeterSetting\" TEXT, \n",
    "\t\"HourlyDewPointTemperature\" TEXT, \n",
    "\t\"HourlyDryBulbTemperature\" TEXT, \n",
    "\t\"HourlyPrecipitation\" TEXT, \n",
    "\t\"HourlyPresentWeatherType\" TEXT, \n",
    "\t\"HourlyPressureChange\" TEXT, \n",
    "\t\"HourlyPressureTendency\" FLOAT, \n",
    "\t\"HourlyRelativeHumidity\" FLOAT, \n",
    "\t\"HourlySkyConditions\" TEXT, \n",
    "\t\"HourlySeaLevelPressure\" TEXT, \n",
    "\t\"HourlyStationPressure\" TEXT, \n",
    "\t\"HourlyVisibility\" TEXT, \n",
    "\t\"HourlyWetBulbTemperature\" FLOAT, \n",
    "\t\"HourlyWindDirection\" TEXT, \n",
    "\t\"HourlyWindGustSpeed\" FLOAT, \n",
    "\t\"HourlyWindSpeed\" FLOAT\n",
    ")\n",
    "\n",
    ";\n",
    "\n",
    "\n",
    "\n",
    "CREATE TABLE IF NOT EXISTS uber (\n",
    "\t\"Unnamed: 0.1\" BIGINT, \n",
    "\t\"Unnamed: 0\" BIGINT, \n",
    "\t\"key\" DATETIME, \n",
    "\tfare_amount FLOAT, \n",
    "\tpickup_datetime TIMESTAMP, \n",
    "\tpickup_longitude FLOAT, \n",
    "\tpickup_latitude FLOAT, \n",
    "\tdropoff_longitude FLOAT, \n",
    "\tdropoff_latitude FLOAT, \n",
    "\tpassenger_count BIGINT, \n",
    "\tdistance FLOAT\n",
    ")\n",
    "\n",
    ";\n",
    "\n",
    "\n",
    "\n",
    "CREATE TABLE IF NOT EXISTS yellow_taxi (\n",
    "\t\"Unnamed: 0\" BIGINT, \n",
    "\tvendor_id TEXT, \n",
    "\tpickup_datetime DATETIME, \n",
    "\tdropoff_datetime DATETIME, \n",
    "\tpassenger_count SMALLINT, \n",
    "\ttrip_distance FLOAT, \n",
    "\tpickup_longitude FLOAT, \n",
    "\tpickup_latitude FLOAT, \n",
    "\tdropoff_longitude FLOAT, \n",
    "\tdropoff_latitude FLOAT, \n",
    "\tpayment_type TEXT, \n",
    "\ttip_amount FLOAT, \n",
    "\ttolls_amount FLOAT, \n",
    "\ttotal_amount FLOAT, \n",
    "\tfare_amount FLOAT, \n",
    "\tdistance FLOAT\n",
    ")\n",
    ";\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "with open(\"schema.sql\", \"w\") as schema_file:\n",
    "    schema_file.write(create_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
